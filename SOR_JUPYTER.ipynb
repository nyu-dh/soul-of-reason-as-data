{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should only need to run this line once; you can comment it out after running: \n",
    "\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def import_txt_files(self, path):\n",
    "        \"\"\" \n",
    "        \n",
    "        Loads a series of text files into Python. \n",
    "    \n",
    "        Input\n",
    "        -----------------------------------------\n",
    "        path : string\n",
    "            This is the directory path to your folder of text files. Note that the path must be in single ('') \n",
    "            or double (\"\") quotations. To avoid confusion during processing, this directory should only contain\n",
    "            the text files that you want to analyze for entities.\n",
    "        \n",
    "            Example - \n",
    "        \n",
    "                    '/Users/myusername/Desktop/soul-of-reason-as-data/data' \n",
    "                    (where /data is the folder containing all text files)\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -----------------------------------------\n",
    "        corpus : list \n",
    "            Returns a Python list of all of the text files, as well as the total number of documents in the corpus \n",
    "            (the number of text files in your folder).\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Importing text files...\")\n",
    "        \n",
    "        # Compile all text file names from working directory\n",
    "        \n",
    "        txt_files = glob.glob(os.path.join(os.getcwd(), path, \"*.txt\"))\n",
    "        \n",
    "        # Append each document to a list called corpus + error-handle\n",
    "\n",
    "        corpus = []\n",
    "        \n",
    "        try:\n",
    "            for individual_file in txt_files:\n",
    "                with open(individual_file) as f_input:\n",
    "                    corpus.append(f_input.read())\n",
    "        except:\n",
    "            print(\"ERROR: Could not import text files. Please ensure that you entered that correct directory path \"+\n",
    "                  \"and that all of the files you want to analyze have a .txt extension.\")\n",
    "            \n",
    "                \n",
    "        # Return the number of documents in the corpus & the Python corpus list\n",
    "\n",
    "        print(\"Number of documents in corpus: \",len(corpus))\n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    \n",
    "    def remove_timestamps(self, docs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Removes timestaps (i.e. - 'Dr. Roscoe C Brown 00:00:01.490') from text. This ensures that the model does \n",
    "        not repeatedly include these entities in the final dataset. \n",
    "        \n",
    "        Input \n",
    "        -----------------------------------------\n",
    "        text : list\n",
    "            The list of documents you want to remove timestamps from; i.e. the output from import_txt_files().\n",
    "            \n",
    "            \n",
    "        Returns\n",
    "        -----------------------------------------\n",
    "        docs : list\n",
    "            The same list of documents from the input, except with all timestamps removed. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Removing timestamps...\")\n",
    "        \n",
    "        # Find all transcription lines using regex (must convert to string/non-list item first)\n",
    "        \n",
    "        text = ''.join(docs)\n",
    "        lines = re.findall(\"[0-9][0-9].[0-9][0-9].[0-9][0-9].[0-9][0-9][0-9]*\", text)\n",
    "        \n",
    "        # Error handle -- return message if there are no transcription lines to be found\n",
    "        \n",
    "        if len(lines) == 0:\n",
    "            print(\"ERROR: No transcription lines have been found in the corpus. Did you remove them already? If so, \"+\n",
    "                 \"you don't need to use this function and can skip to link_metadata.\")\n",
    "        else:\n",
    "            \n",
    "            # Split lines + replaced double-spaced lines with single spaced lines\n",
    "\n",
    "            split = text.replace('\\n\\n','\\n').splitlines() \n",
    "\n",
    "            # Append all of the transcription lines to a list\n",
    "\n",
    "            lines_remove = []                  \n",
    "            for i in split:\n",
    "                for j in lines:\n",
    "                    if j in i:\n",
    "                        lines_remove.append(i)\n",
    "\n",
    "            # Remove aforementioned trancription lines\n",
    "\n",
    "            for i in range(len(docs)):          \n",
    "                for j in lines_remove:\n",
    "                    if j in docs[i]:\n",
    "                        docs[i] = docs[i].replace(j,'')\n",
    "\n",
    "            # Replace double-lines with single & triple lines with double\n",
    "\n",
    "            for i in range(len(docs)):\n",
    "                docs[i] = docs[i].replace('\\n\\n','')    \n",
    "\n",
    "            print(\"Timestamps removed from corpus.\")       \n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    \n",
    "    def link_metadata(self, docs, txt_path):\n",
    "        \"\"\"\n",
    "        \n",
    "        Links the text file name (e.g. 'RG_9_8_208.txt') to the corresponding text document in Python.\n",
    "\n",
    "        NOTE: \n",
    "            - This function is dependent on the fact that each text file is named in a format similar to \n",
    "             'RG_9_8_208.txt' or 'RG9-8_ref4330.txt' (for example). Text files that do not meet either of \n",
    "              these naming formats will not be identified and an error will be thrown, with a reference to \n",
    "              the invalid document(s).\n",
    "\n",
    "            - Please also ensure that the text files in your txt_path are only those you want to analyze \n",
    "              (i.e. no irrelevant text files).\n",
    "\n",
    "            - Running this function multiple times in the same class instance will result in the metadata \n",
    "              being appended more than once. \n",
    "\n",
    "        Input\n",
    "        -------------------------\n",
    "        docs : list\n",
    "            The list of all documents where metadata needs to be appended, i.e. the 'docs' output from \n",
    "            remove_timestamps(). The number of documents in this list should match the number of documents \n",
    "            imported after using import_txt_files().  \n",
    "\n",
    "        txt_path : string\n",
    "            The directory path on your computer where all .txt files are stored; this should be the same as that \n",
    "            used in import_txt_files(). Note that the path must be in single ('') or double (\"\") quotations.\n",
    "\n",
    "            Example - \n",
    "\n",
    "            '/Users/myusername/Desktop/soul-of-reason-as-data/data' \n",
    "            (where /data is the folder containing all text files)\n",
    "\n",
    "        Returns\n",
    "        ------------------------\n",
    "        docs : list\n",
    "            The same list you inputted, except with an appended reference to the corresponding file name at the end \n",
    "            of every document, in the form of: <DOC_ID>: 'text file name.'\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Adding metadata identifier to each document...\\n\\n\")\n",
    "        \n",
    "        # Create list of text file names from txt_path\n",
    "        \n",
    "        txt_files = glob.glob(os.path.join(os.getcwd(), txt_path, \"*.txt\"))\n",
    "        \n",
    "        # Create four empty lists to collect the file names for each document\n",
    "        # There are four lists because there are four 'types' of naming formats\n",
    "        # l1 will be the final list (other lists will be merged into this one)\n",
    "        # Therefore, make l1 a global variable because it will be used in another function\n",
    "\n",
    "        global l1 \n",
    "        l1 = []\n",
    "        l2 = []\n",
    "        l3 = []\n",
    "        l4 = []\n",
    "\n",
    "        # Find all names of text files using regex, following the given formats (mentioned in docstring)\n",
    "        \n",
    "        for i in range(len(txt_files)):   \n",
    "            l1.append(re.findall('[A-Z][A-Z]\\w[0-9]\\w[0-9]\\w[0-9][0-9][0-9]\\W[a-z][a-z][a-z]',txt_files[i]))\n",
    "            l2.append(re.findall('[A-Z][A-Z]\\w[0-9]\\w[0-9]\\w[0-9][0-9]\\W[a-z][a-z][a-z]',txt_files[i]))\n",
    "            l3.append(re.findall('[A-Z][A-Z][0-9]\\W[0-9]\\w[a-z][a-z][a-z][0-9][0-9][0-9][0-9]\\W[a-z][a-z][a-z]', txt_files[i]))\n",
    "            l4.append(re.findall('[A-Z][A-Z][0-9]\\w[0-9]\\w[0-9][0-9][0-9]\\W[a-z][a-z][a-z]',txt_files[i]))\n",
    "        \n",
    "        # Merge the lists of the text file names to get one complete list, l1\n",
    "        \n",
    "        for i in range(len(l1)):         \n",
    "            if l1[i] == [] and l2[i] != []:\n",
    "                l1[i] = l2[i]\n",
    "            elif l1[i] == [] and l3[i] != []:\n",
    "                l1[i] = l3[i]\n",
    "            elif l1[i] == [] and l4[i] != []:\n",
    "                l1[i] = l4[i]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # Add the corresponding .txt file name to the end of each document (+ error handling)\n",
    "        # A message will appear if files do not file the given naming format (as given by the digi_id of metadata)\n",
    "        # A message will appear if you have extra text files in your directory path\n",
    "        # This <DOC_ID> will be called to link the metadata file when creating the final dataset\n",
    "\n",
    "        empties = []    \n",
    "        if len(docs) == len(l1):\n",
    "            for i in tqdm(range(len(docs))):\n",
    "                docs[i] = docs[i] + ' <DOC_ID>: '+''.join(l1[i])\n",
    "            print(\"Finished linking metadata. Each document in the corpus list contains a <DOC_ID> identifier on the last \"\\\n",
    "                  \"line, with its corresponding .txt file name.\\n\\n\")\n",
    "        else:\n",
    "            if [] in l1:\n",
    "                for i in range(len(l1)):\n",
    "                    if l1[i] == []:\n",
    "                        empties.append(i)\n",
    "                    empty_docs = [txt_files[x] for x in empties]\n",
    "                print(\"ERROR: One or more .txt document names could not be found. Please ensure that the names of all .txt \"+\n",
    "                \"files follow the permitted naming format. Error comes from document(s): \",empties,empty_docs)\n",
    "            else:\n",
    "                print(\"ERROR: The number of text files in your directory path does not equal the number of text files imported!\\n\"+\n",
    "                      \"Please ensure there are no additional .txt files in the file path you supplied.\\n\\n\")\n",
    "\n",
    "        return docs\n",
    "    \n",
    "\n",
    "    def predict_entities(self, corpus):\n",
    "        \"\"\"\n",
    "        \n",
    "        Predicts entities in the given corpus. Entities are labelled according to spaCy entitiy types; see \n",
    "        https://spacy.io/api/annotation#named-entities for a list of all labels.\n",
    "        \n",
    "        NOTE:\n",
    "            - Remember that this function has two outputs (described below). The next function takes only the FIRST\n",
    "              ouptut of this function, therefore you will need to index this function's output by [0].\n",
    "        \n",
    "        Input\n",
    "        ----------------------------------------\n",
    "        corpus : list\n",
    "            The list of documents to extract entities from; this should be the output from link_metadata().\n",
    "            \n",
    "        Returns\n",
    "        ----------------------------------------\n",
    "        preds : list\n",
    "            A list of tuples in the form of [(Entity, Start_Index, End_Index, Entity_Label, Doc_ID)], predicted \n",
    "            by the spaCy model. This will be passed as input to the create_final_dataset() function.\n",
    "            \n",
    "        ents_only : list\n",
    "            A list of *only* the predicted entities; includes duplicates and predicted entities from every document.\n",
    "            This output is not used in any other function; it is returned as a supplementary list that could be \n",
    "            useful for further analysis in Python.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Predicting entitiies...\")\n",
    "        \n",
    "        # Extract <DOC_ID> tokens & append to list, doc_ids\n",
    "        # Use the index -17 because the <DOC_ID> token will be the very last token (hence, the negative)\n",
    "        # 17 is used because, according to the given format, the longest document id would be 17 characters\n",
    "        \n",
    "        preds = []\n",
    "        doc_ids = []\n",
    "        ents_only = []\n",
    "        \n",
    "        for i in range(len(corpus)):\n",
    "            doc = corpus[i]\n",
    "            doc_id = doc[-17:]\n",
    "            doc_ids.append(doc_id)\n",
    "        \n",
    "        # Remove extra characters from doc_ids list --> remove 'D>:' and '>:' which may have been included\n",
    "        # ... becauase of the <DOC_ID> part\n",
    "        # This applies to file names that are less than 17 characters \n",
    "        \n",
    "        for i in range(len(doc_ids)):\n",
    "            doc_ids[i] = doc_ids[i].replace('D>: ','')\n",
    "            doc_ids[i] = doc_ids[i].replace('>: ','')\n",
    "        \n",
    "        # Predict entities, append information to a list called preds + error handling\n",
    "        # Predictions made using spaCy's pre-trained model, which I named 'nlp' (see the init method) \n",
    "        \n",
    "        try:\n",
    "            for i in tqdm(range(len(corpus))):\n",
    "                doc = corpus[i]\n",
    "                preproc_doc = self.nlp(doc)\n",
    "                for ent in preproc_doc.ents:\n",
    "                    preds.append((ent.text, ent.start_char, ent.end_char, ent.label_, doc_ids[i]))\n",
    "                    ents_only.append(ent.text)\n",
    "        except:\n",
    "            print(\"ERROR: Could not predict all entities. Please ensure you are using the correct input to this function.\\n\\n\")\n",
    "\n",
    "\n",
    "        return preds, ents_only\n",
    "\n",
    "    \n",
    "    def create_final_dataset(self, preds, metadata_path, *args):\n",
    "        \"\"\"\n",
    "        \n",
    "        Creates the final dataset of named entities, and downloads this dataset to your current working directory\n",
    "        as both a csv and json file. \n",
    "        \n",
    "        NOTE:\n",
    "            - This function requires the column containing the text file names in the metadata file to be \n",
    "              called 'digi_id'\n",
    "            - If you choose to include the *args option (see description below), note that it must come \n",
    "              after the 'metadata_path' argument. \n",
    "            - Remember that you should only be using the first output from predict_entities() as input to this\n",
    "              function. This means you need to index the ouptut from predict_entities() by [0].\n",
    "        \n",
    "        Input\n",
    "        ----------------------------------\n",
    "        preds : list (required)\n",
    "            The list of tuples in the form of [(Entity, Start_Index, End_Index, Entity_Label, Doc_ID)]. This should\n",
    "            be the output from the predict_entities() function.\n",
    "            \n",
    "        metadata_path : string (required)\n",
    "            The file path containing the metadata file of your corpus. Note that this should be the *file* path and \n",
    "            not the *folder* path. \n",
    "            \n",
    "        *args : list (optional)\n",
    "            The list of entity types you want to filter by. This is an optional argument that allows you to choose \n",
    "            which entity types you want to see; i.e. - ['PERSON', 'ORG'] will return/save a dataset containing only\n",
    "            entities identified as a person or organization. \n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        ----------------------------------\n",
    "        final_df : pandas DataFrame\n",
    "            The final dataset of named entities, containing the entity, its start & end Python index, its entity \n",
    "            type, and all corresponding inputted metadata from the metadata file path.\n",
    "            \n",
    "        This function also returns both a csv and a json file with the same information as that in final_df above. \n",
    "        These datasets are automatically downloaded to your current working directory.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Creating final dataset...\\n\\n\")\n",
    "        \n",
    "        # Make dataset, name columns\n",
    "        \n",
    "        cols = ['Entities','Start_index','End_index','Label','Doc_ID']\n",
    "        df = pd.DataFrame(preds,columns=cols)\n",
    "        \n",
    "        # Drop 'DOC_ID' string & possible text file names picked up as entities\n",
    "        # For example, 'RG_9_8_110' may be a record in the dataset labelled as CARDINAL\n",
    "        \n",
    "        df = df[~df['Entities'].isin(['DOC_ID'])]\n",
    "        df = df[~df['Entities'].isin(l1)]\n",
    "        \n",
    "        # Drop '.txt' at the end of Doc_ID column, so that only the digi ids are reflected\n",
    "        \n",
    "        df['Doc_ID'] = df['Doc_ID'].replace(to_replace=r'\\.txt', value='', regex=True)\n",
    "        \n",
    "        # Read in metadata file\n",
    "        \n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Option to filter dataset by entitiy type\n",
    "        \n",
    "        if args:\n",
    "            my_ents = list(chain(*args))\n",
    "            try:\n",
    "                df = df[df['Label'].isin(my_ents)]\n",
    "            except:\n",
    "                print(\"ERROR: could not filter by the inputted entitiy types. Please ensure each of your entity \"+\n",
    "                     \"types is correct & exactly matches the spaCy formatting. See \"+\n",
    "                      \"https://spacy.io/api/annotation#named-entities for a list of all available entity types.\\n\\n\")\n",
    "        \n",
    "        # Link metadata file with entity dataset\n",
    "        \n",
    "        try:\n",
    "            metadata = metadata.rename(columns={\"digi_id\":'Doc_ID'})\n",
    "            final_df = pd.merge(df, metadata, how='inner', on='Doc_ID')\n",
    "        except:\n",
    "            print(\"ERROR: Could not link metadata to entity dataset. Please ensure that the column name 'digi_id' (the \"+\n",
    "                 \"column containg the text file names), is present in the metadata file.\\n\\n\")\n",
    "        \n",
    "        # Save dataset as csv to current directory\n",
    "        \n",
    "        final_df.to_csv('named_entities_csv.csv',index=False)\n",
    "        final_df.to_json('named_entities_json.json', orient='columns')\n",
    "        \n",
    "        print(\"The named entity datastets, 'named_entities_csv' and 'named_entities_json', have been saved to your \"+\n",
    "              \"current working \\ndirectory.\\n\\n\")\n",
    "        \n",
    "        # Give number of unique labels identified\n",
    "        \n",
    "        print(\"\\nNumber of unique entities identified:\",len(final_df['Entities'].unique()),\"\\n\\n\\nNumber of entities \"+ \n",
    "              \"identified, by entitiy type:\\n\",final_df['Label'].value_counts())\n",
    "         \n",
    "        return final_df\n",
    "        \n",
    "    \n",
    "    def display_entities(self, path, *args): \n",
    "        \"\"\"\n",
    "        Displays highlighted entities of a specific document, with the option to view only specific entitiy types.\n",
    "        \n",
    "        NOTE: \n",
    "        \n",
    "            - If you decide to export the Jupyter notebook, the visualizations are included as HTML.\n",
    "            \n",
    "            - If you choose to include the *args option, note that it must come after the 'path' argument. \n",
    "        \n",
    "        Input\n",
    "        -------------------\n",
    "        path : string (required)\n",
    "            The *file* path of the specific document you want to view. Note that the file path must be in \n",
    "            single ('') or double (\"\") quotations.\n",
    "            \n",
    "            Example - \n",
    "            \n",
    "            '/Users/myusername/Desktop/soul-of-reason-as-data/data/RG_9_8_110.txt'\n",
    "            \n",
    "        *args : list (optional)\n",
    "            The list of entity types you would like to view/filter. If you do not include this argument, the\n",
    "            visualization will include all entity types. \n",
    "            \n",
    "            Example - \n",
    "                \n",
    "                To view only the 'Person','Organization', and 'Date' entity types, you would enter:\n",
    "                ['PERSON', 'ORG', 'DATE']\n",
    "                \n",
    "                Please see https://spacy.io/api/annotation#named-entities for the complete list of entity types.\n",
    "                Note that all entity-types must be comma-separated, in (single or double) quotations and typed \n",
    "                exactly as shown in the provided link. To view only one entity type, simply enter, for example:\n",
    "                ['PERSON']\n",
    "            \n",
    "        Returns\n",
    "        -------------------\n",
    "        html : HTML\n",
    "            HTML visualization of the document with all entities highlighted.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Read in text file \n",
    "        \n",
    "        corpus = []\n",
    "        with open(path) as f_input:\n",
    "            corpus.append(f_input.read())\n",
    "        \n",
    "        # Predict entities \n",
    "        \n",
    "        txt = ''.join(corpus)\n",
    "        txt2 = self.nlp(txt)\n",
    "        \n",
    "        global html\n",
    "        \n",
    "        # Filter by entitiy type(s) if 'args' argument is passed, return visualization (+ error handling)\n",
    "        \n",
    "        if args:\n",
    "            ents = args[0]\n",
    "            if type(ents) != list:\n",
    "                print(\"ERROR: Cannot produce visualization. Please ensure you are entering the entities in a list format.\\n\"+\n",
    "                \"Example- ['PERSON','NORP','FAC']. See: https://spacy.io/api/annotation#named-entities for a complete list of entity \\n\"+\n",
    "                      \"types & enter them in a comma-separated list format with quotations.\")\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    options = {\"ents\": ents}\n",
    "                    txt2.user_data[\"title\"] = \"Entities for:\", path\n",
    "                    html = displacy.render(txt2, style=\"ent\",options=options, page=True, minify=True)\n",
    "                except:\n",
    "                    print(\"ERROR: Cannot produce visualization. Please ensure the entities you specified are correctly entered.\\n\"+\n",
    "                          \"See: https://spacy.io/api/annotation#named-entities for a complete list of entity types & \\n\"+\n",
    "                          \"enter them in a comma-separated list format with quotations (example- ['PERSON','NORP','FAC']).\")\n",
    "        else:\n",
    "            txt2.user_data[\"title\"] = \"Entities for:\", path\n",
    "            html = displacy.render(txt2, style=\"ent\", page=True, minify=True)\n",
    "        \n",
    "        return html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = NER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp1.import_txt_files('INSERT YOUR TEXT FOLDER PATH HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = nlp1.remove_timestamps(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_text = nlp1.link_metadata(clean_text, 'INSERT YOUR TEXT FOLDER PATH HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = nlp1.predict_entities(met_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nlp1.create_final_dataset(ents[0],'INSERT METADATA FILE PATH HERE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
